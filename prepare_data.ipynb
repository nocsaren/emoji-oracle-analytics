{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e98b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# --- Google Cloud Auth + APIs ---\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError\n",
    "\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "# --- Data & Visualization ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "\n",
    "# --- Local Modules ---\n",
    "\n",
    "from modules.utilities import (\n",
    "    pull_and_append,\n",
    "#    rebuild_data_json_from_backups,\n",
    "    upload_named_dataframes_to_bq\n",
    ")\n",
    "\n",
    "from modules.flattening import (\n",
    "    flatten_extract_params, \n",
    "    flatten_row,\n",
    "    flatten_nested_column\n",
    ")\n",
    "\n",
    "from modules.cleaning import (\n",
    "    apply_value_maps,\n",
    "    safe_select_and_rename\n",
    ")\n",
    "# --- Lists and Maps ---\n",
    "\n",
    "from modules.lists_and_maps import (\n",
    "    df_column_names_map, \n",
    "    columns_to_drop,\n",
    "    map_of_maps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f809a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Path Setup ---\n",
    "SERVICE_ACCOUNT_KEY = './keys/key.json'\n",
    "DATA_PATH = './data/data.json'\n",
    "PROJECT_ID = \"emojioracle-342f1\"\n",
    "DATASET_ID = \"analytics_481352676\"\n",
    "BACKUP_PATH = './backup/'\n",
    "\n",
    "# --- BigQuery Setup ---\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/bigquery\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_KEY,\n",
    "    scopes = SCOPES\n",
    ")\n",
    "bq_client = bigquery.Client(credentials = credentials, project = PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8796488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "raw_data = pd.DataFrame(pull_and_append(credentials = credentials, \n",
    "                                  project_id = PROJECT_ID, \n",
    "                                  dataset_id = DATASET_ID, \n",
    "                                  data_path = DATA_PATH, \n",
    "                                  backup_path = BACKUP_PATH))\n",
    "\n",
    "print(f\"Data loaded with {len(raw_data)} rows and {len(raw_data.columns)} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data into a DataFrame\n",
    "df = pd.read_json(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # uncomment to see all of the cols in pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Flatten the DataFrame ---\n",
    "df = pd.DataFrame([flatten_row(row) for _, row in df.iterrows()]) # for wtfs refer to ./modules/flattening_json.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50760f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace('.', '__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a96e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Date and Time Cleanup and Transformation ---\n",
    "df = df.drop(columns=['event_date']) # built in case event_date may not be the same as the one in the event_timestamp\n",
    "\n",
    "df['time_delta'] = pd.to_datetime(df['event_timestamp'], unit='us', utc=True) - pd.to_datetime(df['event_previous_timestamp'], unit='us', utc=True)\n",
    "df['time_delta'] = df['time_delta'].dt.total_seconds() # convert to seconds\n",
    "\n",
    "df['event_datetime'] = pd.to_datetime(df['event_timestamp'], unit='us', utc=True) \n",
    "df['event_previous_datetime'] = pd.to_datetime(df['event_previous_timestamp'], unit='us', utc=True)\n",
    "df['event_first_touch_datetime'] = pd.to_datetime(df['user_first_touch_timestamp'], unit='us', utc=True)\n",
    "df['user__first_open_datetime'] = pd.to_datetime(df['user__first_open_time'], unit='ms', utc=True)\n",
    "\n",
    "\n",
    "df['event_date'] = df['event_datetime'].dt.normalize()\n",
    "df['event_time'] = df['event_datetime'].dt.time\n",
    "\n",
    "df['event_previous_date'] = df['event_previous_datetime'].dt.normalize()\n",
    "df['event_previous_time'] = df['event_previous_datetime'].dt.time\n",
    "\n",
    "df['event_first_touch_date'] = df['event_first_touch_datetime'].dt.normalize()\n",
    "df['event_first_touch_time'] = df['event_first_touch_datetime'].dt.time\n",
    "\n",
    "df['user__first_open_date'] = df['user__first_open_datetime'].dt.normalize()\n",
    "df['user__first_open_time'] = df['user__first_open_datetime'].dt.time\n",
    "\n",
    "df['device__time_zone_offset_hours'] = df['device__time_zone_offset_seconds'] / 3600 # seconds to hours\n",
    "df['event_params__engagement_time_seconds'] = df['event_params__engagement_time_msec'] / 1000 # ms to seconds\n",
    "df['event_server_delay_seconds'] = df['event_server_timestamp_offset'] / 1000 # ms to seconds \n",
    "df['event_params__time_spent_seconds'] = df['event_params__time_spent'] # just renaming for clarity\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb00329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add Time-Based Features ---\n",
    "\n",
    "df['ts_weekday'] = df['event_datetime'].dt.day_name() # weekday name\n",
    "df['ts_weekday'] = pd.Categorical(df['ts_weekday'], \n",
    "                                  categories=['Monday', 'Tuesday', 'Wednesday', \n",
    "                                              'Thursday', 'Friday', 'Saturday', \n",
    "                                              'Sunday'],\n",
    "                                  ordered=True) # order the weekdays\n",
    "\n",
    "df['ts_local_time'] = df['event_datetime'] + pd.to_timedelta(df['device__time_zone_offset_hours'].fillna(0), unit='h') # local time\n",
    "df['ts_hour'] = df['ts_local_time'].dt.hour # local hour\n",
    "df['ts_daytime_named'] = df['ts_hour'].apply(lambda x: \n",
    "                                             'Gece' if (x < 6 or x > 22) else \n",
    "                                             'Sabah' if x < 11 else \n",
    "                                             'Öğle' if x < 14 else \n",
    "                                             'Öğleden Sonra' if x < 17 else \n",
    "                                             'Akşam') # time group of day\n",
    "df['ts_is_weekend'] = df['ts_weekday'].apply(lambda x: \n",
    "                                             'Hafta Sonu' if x in ['Saturday', 'Sunday'] else\n",
    "                                             'Hafta İçi') \n",
    "df['ts_weekday'] = df['ts_weekday'].astype(str) # convert to string for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cce0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Question Index Clean-up ---\n",
    "\"\"\"\n",
    "Tier 1: 16 Questions, Except t: 12\n",
    "Tier 2: 12 Questions\n",
    "Tier 3: 12 Questions\n",
    "Tier 4: 10 Questions\n",
    "\"\"\"\n",
    "\n",
    "df['event_params__current_question_index'] = pd.Series([pd.NA] * len(df), dtype=\"Int64\")\n",
    "\n",
    "df['event_params__current_tier'] = pd.to_numeric(df['event_params__current_tier'], errors='coerce').astype(\"Int64\")\n",
    "df['event_params__current_qi'] = pd.to_numeric(df['event_params__current_qi'], errors='coerce').astype(\"Int64\")\n",
    "\n",
    "notna_mask = df['event_params__character_name'].notna() & df['event_params__current_tier'].notna() & df['event_params__current_qi'].notna()\n",
    "\n",
    "# Tier 1\n",
    "tier_1_mask = notna_mask & (df['event_params__current_tier'] == 1)\n",
    "t_char_mask = tier_1_mask & (df['event_params__character_name'] == 't')\n",
    "\n",
    "df.loc[t_char_mask, 'event_params__current_question_index'] = 13 - df.loc[t_char_mask, 'event_params__current_qi']\n",
    "df.loc[~t_char_mask & tier_1_mask, 'event_params__current_question_index'] = 17 - df.loc[(~t_char_mask) & tier_1_mask, 'event_params__current_qi']\n",
    "\n",
    "# Tier 2 & 3\n",
    "tier_2_3_mask = notna_mask & df['event_params__current_tier'].isin([2, 3])\n",
    "df.loc[tier_2_3_mask, 'event_params__current_question_index'] = 13 - df.loc[tier_2_3_mask, 'event_params__current_qi']\n",
    "\n",
    "# Tier 4\n",
    "tier_4_mask = notna_mask & (df['event_params__current_tier'] == 4)\n",
    "df.loc[tier_4_mask, 'event_params__current_question_index'] = 11 - df.loc[tier_4_mask, 'event_params__current_qi']\n",
    "\n",
    "# Hiccups\n",
    "problems_mask = notna_mask & ~df['event_params__current_tier'].isin([1, 2, 3, 4])\n",
    "if df[problems_mask].shape[0] > 0:\n",
    "    print(\"Something wrong in:\")\n",
    "    print(df.loc[problems_mask, ['event_params__character_name', 'event_params__current_tier', 'event_params__current_qi']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative question index\n",
    "\n",
    "df['cumulative_question_index'] = df['event_params__current_question_index']\n",
    "\n",
    "\n",
    "# Tier 2\n",
    "df.loc[(df['event_params__current_tier'] == 2) & (df['event_params__character_name'] == 't'), 'cumulative_question_index'] += 12\n",
    "df.loc[(df['event_params__current_tier'] == 2) & (df['event_params__character_name'] != 't'), 'cumulative_question_index'] += 16\n",
    "\n",
    "# Tier 3\n",
    "df.loc[(df['event_params__current_tier'] == 3) & (df['event_params__character_name'] == 't'), 'cumulative_question_index'] += 24\n",
    "df.loc[(df['event_params__current_tier'] == 3) & (df['event_params__character_name'] != 't'), 'cumulative_question_index'] += 28\n",
    "\n",
    "# Tier 4\n",
    "df.loc[(df['event_params__current_tier'] == 4) & (df['event_params__character_name'] == 't'), 'cumulative_question_index'] += 36\n",
    "df.loc[(df['event_params__current_tier'] == 4) & (df['event_params__character_name'] != 't'), 'cumulative_question_index'] += 40\n",
    "\n",
    "# NaNs\n",
    "df.loc[df['event_params__current_tier'].isna(), 'cumulative_question_index'] = pd.NA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f903e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Session Definition and Duration Calculation ---\n",
    "\n",
    "''' \n",
    "\n",
    "Create a calculated session times dataframe from the events dataframe.\n",
    "This will infer session times based on the time gaps between events for each user.\n",
    "\n",
    "This is done by:\n",
    "1. Sorting events by user and timestamp.\n",
    "2. Calculating the time difference between consecutive events for each user.\n",
    "3. Defining a session timeout (6 minutes).\n",
    "4. Assigning session IDs based on the time gaps.\n",
    "\n",
    "'''\n",
    "\n",
    "# Ensure events are sorted per user\n",
    "df_sorted = df.sort_values(by=['user_pseudo_id', 'event_datetime'])\n",
    "\n",
    "# Compute time gap between events per user\n",
    "df_sorted['time_diff'] = df_sorted.groupby('user_pseudo_id')['event_datetime'].diff()\n",
    "\n",
    "# Use 6-minute timeout\n",
    "SESSION_TIMEOUT = pd.Timedelta(minutes=6)\n",
    "\n",
    "# Define inferred session ID using 6-minute gaps\n",
    "df_sorted['inferred_session_id'] = (\n",
    "    (df_sorted['time_diff'] > SESSION_TIMEOUT) | df_sorted['time_diff'].isna()\n",
    ").cumsum()\n",
    "\n",
    "# Assign session IDs to the original DataFrame\n",
    "df['inferred_session_id'] = df_sorted['inferred_session_id']\n",
    "\n",
    "# Calculate session duration\n",
    "df['session_duration_seconds'] = df.groupby(['user_pseudo_id', 'inferred_session_id'])['event_datetime'].transform(\n",
    "    lambda x: (x.max() - x.min()).total_seconds()\n",
    ")\n",
    "\n",
    "# Session start and end times\n",
    "df['session_start_time'] = df.groupby(['user_pseudo_id', 'inferred_session_id'])['event_datetime'].transform('min')\n",
    "df['session_end_time'] = df.groupby(['user_pseudo_id', 'inferred_session_id'])['event_datetime'].transform('max')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a03fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer and forward-fill the character name, current tier, and current question index within each session\n",
    "\n",
    "# Step 1: Sort chronologically within sessions\n",
    "df_sorted = df.sort_values(by=['user_pseudo_id', 'inferred_session_id', 'event_datetime'])\n",
    "\n",
    "# Step 2: Forward-fill the relevant columns per user-session group\n",
    "cols_to_fill = [\n",
    "    'event_params__character_name',\n",
    "    'event_params__current_tier',\n",
    "    'event_params__current_question_index'\n",
    "]\n",
    "\n",
    "df_sorted[cols_to_fill] = (\n",
    "    df_sorted\n",
    "    .groupby(['user_pseudo_id', 'inferred_session_id'])[cols_to_fill]\n",
    "    .ffill()\n",
    ")\n",
    "\n",
    "df[cols_to_fill] = df_sorted[cols_to_fill]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'event_params_mini_game_ri' maze_hand_* into columns\n",
    "# e.g 'maze_hand_WomanHandTwo_maze_level_3'\n",
    "\n",
    "# Column to process\n",
    "col = 'event_params__mini_game_ri'\n",
    "\n",
    "# Filter rows starting with 'maze_hand'\n",
    "mask = df[col].str.startswith('maze_hand', na=False)\n",
    "\n",
    "# Split the matching rows by underscore\n",
    "parts = df.loc[mask, col].str.split('_', expand=True)\n",
    "\n",
    "# Extract Gender and Hand using the updated regex\n",
    "gender_hand = parts[2].str.extract(r'(?P<Gender>Woman|Man)Hand(?P<Hand>\\w+)')\n",
    "\n",
    "# Extract Level (assumed to be in the last part)\n",
    "levels = parts[5]\n",
    "\n",
    "# Create new columns with extracted data\n",
    "df.loc[mask, 'maze_gender'] = gender_hand['Gender']\n",
    "df.loc[mask, 'maze_hand'] = gender_hand['Hand']\n",
    "df.loc[mask, 'maze_level'] = levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split event_params_mini_game_ri buff_* into columns\n",
    "# e.g. 'buff_IncreaseXEnergy_gift_True_gold_False'\n",
    "\n",
    "\n",
    "# Column to process\n",
    "col = 'event_params__mini_game_ri'\n",
    "\n",
    "# Filter rows starting with 'buff'\n",
    "mask = df[col].str.startswith('buff', na=False)\n",
    "\n",
    "# Split the matching rows by underscore\n",
    "parts = df.loc[mask, col].str.split('_', expand=True)\n",
    "\n",
    "# Extract Buff Type and Level\n",
    "buff_type = parts[2].str.extract(r'(?P<BuffType>\\w+)')\n",
    "\n",
    "# Extract Buff Gift and Gold status\n",
    "buff_gift = parts[3].str.extract(r'(?P<BuffGift>\\w+)')\n",
    "buff_gold = parts[5].str.extract(r'(?P<BuffGold>\\w+)')\n",
    "\n",
    "# Create new columns with extracted data\n",
    "df.loc[mask, 'buff_type'] = buff_type['BuffType']\n",
    "df.loc[mask, 'buff_gift'] = buff_gift['BuffGift'].str.lower() == 'true'\n",
    "df.loc[mask, 'buff_gold'] = buff_gold['BuffGold'].str.lower() == 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split event_params_mini_game_ri earned_buff_* into columns\n",
    "# e.g. 'earned_buff_GiveXCharacter'\n",
    "\n",
    "# Column to process\n",
    "col = 'event_params__mini_game_ri'\n",
    "\n",
    "# Filter rows starting with 'earned_buff'\n",
    "mask = df[col].str.startswith('earned_buff', na=False)\n",
    "\n",
    "# Split the matching rows by underscore\n",
    "parts = df.loc[mask, col].str.split('_', expand=True)\n",
    "\n",
    "# Extract Buff Type\n",
    "buff_type = parts[2].str.extract(r'(?P<BuffType>\\w+)')\n",
    "\n",
    "# Create new columns with extracted data\n",
    "df.loc[mask, 'earned_buff_type'] = buff_type['BuffType']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split event_params__spent_to doll values into columns\n",
    "# e.g. 'erjohndoll'\n",
    "\n",
    "# Column to process\n",
    "col = 'event_params__spent_to'\n",
    "\n",
    "# Filter rows including string 'doll'\n",
    "mask = df[col].str.contains('doll', na=False)\n",
    "\n",
    "# Split the string by name and doll\n",
    "parts = df.loc[mask, col].str.split('doll', expand=True)\n",
    "\n",
    "# Extract the doll name\n",
    "df.loc[mask, 'doll_name'] = parts[0].str.strip()  # Get the name before 'doll'\n",
    "\n",
    "# Rewrite the 'event_params__spent_to' column to just the doll name\n",
    "df.loc[mask, col] = 'Doll'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split event_params__spent_to crystal values into columns\n",
    "# list of possible values: cauldron_item, aliginn_item, coffee_item\n",
    "\n",
    "# Column to process\n",
    "col = 'event_params__spent_to'\n",
    "\n",
    "# Filter rows including values from the list\n",
    "mask = df[col].str.contains('cauldron_item|aliginn_item|coffee_item', na=False)\n",
    "\n",
    "# Split the string by name and item\n",
    "parts = df.loc[mask, col].str.split('_', expand=True)\n",
    "\n",
    "# Extract the item name\n",
    "df.loc[mask, 'spent_in_crystal'] = parts[0].str.strip()  # Get the name before '_item'\n",
    "\n",
    "# Rewrite the 'event_params__spent_to' column to just the item name\n",
    "df.loc[mask, col] = 'Crystal Ball'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c872a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write event_params_spent_to permanent shop item values into shop_permanent_item\n",
    "# list of possible values: dreamcatcher, catcollar, library1, library2, bugspray, schedule\n",
    "\n",
    "# Column to process\n",
    "col = 'event_params__spent_to'\n",
    "\n",
    "# Filter rows including values from the list\n",
    "mask = df[col].str.contains('dreamcatcher|catcollar|library1|library2|bugspray|schedule|crystal|horseshoe', na=False)\n",
    "\n",
    "# Create a new column for the shop permanent item\n",
    "df.loc[mask, 'shop_permanent_item'] = df.loc[mask, col].str.extract(r'(dreamcatcher|catcollar|library1|library2|bugspray|schedule|crystal|horseshoe)')[0]\n",
    "\n",
    "# Rewrite the 'event_params__spent_to' column to just the item name\n",
    "\n",
    "df.loc[mask, col] = 'Permanent Item'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703893db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write event_params_spent_to consumable shop item values into shop_consumable_item\n",
    "# list of possible values: potion, ıncense, amulet, incense\n",
    "\n",
    "# Column to process\n",
    "col = 'event_params__spent_to'\n",
    "\n",
    "# Filter rows including values from the list\n",
    "mask = df[col].str.contains('potion|ıncense|amulet|incense', na=False)\n",
    "\n",
    "# Create a new column for the shop consumable item\n",
    "df.loc[mask, 'shop_consumable_item'] = df.loc[mask, col].str.extract(r'(potion|ıncense|amulet|incense)')[0]\n",
    "\n",
    "# Rewrite the 'event_params__spent_to' column to just the item name\n",
    "df.loc[mask, col] = 'Consumable Item'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write event_params_spent_to mini_game remainin item values into board_item\n",
    "# everything except: ['Doll', 'Crystal Ball', 'Permanent Item', 'Consumable Item']\n",
    "\n",
    "# Column to process\n",
    "col = 'event_params__spent_to'\n",
    "\n",
    "# Filter rows that are not in the known categories\n",
    "mask = (~df[col].isin(['Doll', 'Crystal Ball', 'Permanent Item', 'Consumable Item'])) & \\\n",
    "    (df['event_params__where_its_spent'].isin(['board', 'board_item']))\n",
    "\n",
    "# Create a new column for the board item\n",
    "df.loc[mask, 'board_item'] = df.loc[mask, col]\n",
    "\n",
    "# Rewrite the 'event_params__spent_to' column to just the item name\n",
    "df.loc[mask, col] = 'Board Item'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb09bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85122e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit rights and wrongs for questions\n",
    "\n",
    "df['question_correct_incorrect'] = df.apply(\n",
    "    lambda row: 'Doğru' if (row['event_name'] == 'question_completed' and pd.isna(row['event_params__answered_wrong'])) else\n",
    "                'Yanlış' if row['event_params__answered_wrong'] == 1 else\n",
    "                pd.NA,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f65261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apply_value_maps(df, map_of_maps, keep_unmapped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301fe170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adressable question index\n",
    "df['question_address'] = df['event_params__character_name'] + ' - T: ' + df['event_params__current_tier'].astype(str) + ' - Q: ' + df['event_params__current_question_index'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399fb386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user_metrics\n",
    "\n",
    "\n",
    "df['event_datetime'] = pd.to_datetime(df['event_datetime'], errors='coerce')\n",
    "\n",
    "# Group by user and calculate user-level metrics\n",
    "user_metrics = df.groupby('user_pseudo_id').agg(\n",
    "    first_seen=('event_datetime', 'min'),\n",
    "    last_seen=('event_datetime', 'max'),\n",
    "    total_sessions=('inferred_session_id', pd.Series.nunique),\n",
    "    total_events=('event_name', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Compute user lifetime in days\n",
    "user_metrics['lifetime_days'] = np.ceil((user_metrics['last_seen'] - \n",
    "                                         user_metrics['first_seen']).dt.total_seconds() / 86400\n",
    ").astype('Int64')  # nullable int type for BQ/LS compatibility\n",
    "\n",
    "# Churn flag based on 80th percentile of days since last event\n",
    "reference_date = df['event_datetime'].max()\n",
    "\n",
    "\n",
    "threshold = user_metrics['lifetime_days'].quantile(0.80)\n",
    "user_metrics['is_churned'] = user_metrics['lifetime_days'] > threshold\n",
    "\n",
    "\n",
    "# Retention buckets (for visualization or filtering in LS)\n",
    "user_metrics['retention_bucket'] = pd.cut(\n",
    "    user_metrics['lifetime_days'],\n",
    "    bins=[-1, 0, 1, 3, 7, 14, 30, 90, float('inf')],\n",
    "    labels=[\n",
    "            '0_0d',\n",
    "            '1_1d',\n",
    "            '2_1-3d',\n",
    "            '3_4-7d',\n",
    "            '4_8-14d',\n",
    "            '5_15-30d',\n",
    "            '6_31-90d',\n",
    "            '7_90+d'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Active/returning user flags\n",
    "user_metrics['is_retained_1d'] = user_metrics['lifetime_days'] >= 1\n",
    "user_metrics['is_retained_7d'] = user_metrics['lifetime_days'] >= 7\n",
    "user_metrics['is_retained_30d'] = user_metrics['lifetime_days'] >= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = safe_select_and_rename(df, df_column_names_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Upload Data to BigQuery ---\n",
    "\n",
    "# Define table names\n",
    "main_table_id = f\"{PROJECT_ID}.{DATASET_ID}.clean_data\"\n",
    "\n",
    "upload_named_dataframes_to_bq(\n",
    "    dataframes={\n",
    "        \"MainCleanData\": df,\n",
    "        \"UserMetrics\": user_metrics,\n",
    "        },\n",
    "    dataset_id=DATASET_ID,\n",
    "    project_id=PROJECT_ID,\n",
    "    bq_client=bq_client,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftz = df.copy()\n",
    "for col in df.select_dtypes(include=['datetimetz']):\n",
    "    df[col] = df[col].dt.tz_localize(None)\n",
    "\n",
    "\n",
    "df.to_excel('./data/cleaned_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(max_cols=1000, memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84112eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event_params__how_its_earned'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d44391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['event_params__how_its_earned'] == 'mini_game_completed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1797b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['event_name'] == 'Menu Closed'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['menu_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff0290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
