{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f51375c4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data update process...\n",
      "Imports completed successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting data update process...\")\n",
    "\n",
    "# --- Standard Library ---\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "# --- Google Cloud Auth + APIs ---\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "# --- Data & Visualization ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "# --- Local Modules ---\n",
    "from modules.utilities import (\n",
    "    pull_and_append,\n",
    "    rebuild_data_json_from_backups,\n",
    "    upload_named_dataframes_to_bq,\n",
    "    convert_bool_to_int\n",
    ")\n",
    "from modules.flattening import (\n",
    "    flatten_extract_params, \n",
    "    flatten_row,\n",
    "    flatten_nested_column\n",
    ")\n",
    "from modules.cleaning import (\n",
    "    apply_value_maps,\n",
    "    safe_select_and_rename\n",
    ")\n",
    "# --- Lists and Maps ---\n",
    "from modules.lists_and_maps import (\n",
    "    df_column_names_map, \n",
    "    columns_to_drop,\n",
    "    map_of_maps,\n",
    "    df_splits,\n",
    "    df_filters\n",
    "    )\n",
    "print(\"Imports completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ced2d15",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set up successfully.\n",
      "BigQuery client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Path Setup ---\n",
    "SERVICE_ACCOUNT_KEY = './keys/key.json'\n",
    "DATA_PATH = './data/data.json'\n",
    "PROJECT_ID = \"emojioracle-342f1\"\n",
    "DATASET_ID = \"analytics_481352676\"\n",
    "BACKUP_PATH = './backup/'\n",
    "# Ensure service account key exists\n",
    "if not os.path.exists(SERVICE_ACCOUNT_KEY):\n",
    "    print(f\"Service account key not found at {SERVICE_ACCOUNT_KEY}. Please check the path, or download a new json key file.\")\n",
    "    sys.exit(1)\n",
    "print(\"Paths set up successfully.\")\n",
    "# --- BigQuery Setup ---\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/bigquery\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_KEY,\n",
    "    scopes = SCOPES\n",
    ")\n",
    "bq_client = bigquery.Client(credentials = credentials, project = PROJECT_ID)\n",
    "print(\"BigQuery client initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9be39",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "raw_data = pd.DataFrame(pull_and_append(credentials = credentials, \n",
    "                                  project_id = PROJECT_ID, \n",
    "                                  dataset_id = DATASET_ID, \n",
    "                                  data_path = DATA_PATH, \n",
    "                                  backup_path = BACKUP_PATH))\n",
    "print(f\"Data loaded with {len(raw_data)} rows and {len(raw_data.columns)} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b390be",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded into DataFrame with 31284 rows and 30 columns.\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data into a DataFrame\n",
    "df = pd.read_json(DATA_PATH)\n",
    "print(f\"Data loaded into DataFrame with {df.shape[0]} rows and {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470bf4ea",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data flattened to 31284 rows and 92 columns.\n"
     ]
    }
   ],
   "source": [
    "# --- Flatten the DataFrame ---\n",
    "df = pd.DataFrame([flatten_row(row) for _, row in df.iterrows()]) # for wtfs refer to ./modules/flattening_json.py\n",
    "print(f\"Data flattened to {df.shape[0]} rows and {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db7b962c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names updated to use '__' instead of '.' - now 92 columns.\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.replace('.', '__')\n",
    "print(f\"Column names updated to use '__' instead of '.' - now {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87dd4b32",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date and time cleanup and transformation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Date and Time Cleanup and Transformation ---\n",
    "df = df.drop(columns=['event_date'], errors='ignore') # built in case event_date may not be the same as the one in the event_timestamp\n",
    "df['time_delta'] = pd.to_datetime(df['event_timestamp'], unit='us', utc=True) - pd.to_datetime(df['event_previous_timestamp'], unit='us', utc=True)\n",
    "df['time_delta'] = df['time_delta'].dt.total_seconds() # convert to seconds\n",
    "df['event_datetime'] = pd.to_datetime(df['event_timestamp'], unit='us', utc=True) \n",
    "df['event_previous_datetime'] = pd.to_datetime(df['event_previous_timestamp'], unit='us', utc=True)\n",
    "df['event_first_touch_datetime'] = pd.to_datetime(df['user_first_touch_timestamp'], unit='us', utc=True)\n",
    "df['user__first_open_datetime'] = pd.to_datetime(df['user__first_open_time'], unit='ms', utc=True)\n",
    "df['event_date'] = df['event_datetime'].dt.normalize()\n",
    "df['event_time'] = df['event_datetime'].dt.time\n",
    "df['event_previous_date'] = df['event_previous_datetime'].dt.normalize()\n",
    "df['event_previous_time'] = df['event_previous_datetime'].dt.time\n",
    "df['event_first_touch_date'] = df['event_first_touch_datetime'].dt.normalize()\n",
    "df['event_first_touch_time'] = df['event_first_touch_datetime'].dt.time\n",
    "df['user__first_open_date'] = df['user__first_open_datetime'].dt.normalize()\n",
    "df['user__first_open_time'] = df['user__first_open_datetime'].dt.time\n",
    "df['device__time_zone_offset_hours'] = df['device__time_zone_offset_seconds'] / 3600 # seconds to hours\n",
    "df['event_params__engagement_time_seconds'] = df['event_params__engagement_time_msec'] / 1000 # ms to seconds\n",
    "df['event_server_delay_seconds'] = df['event_server_timestamp_offset'] / 1000 # ms to seconds \n",
    "df['event_params__time_spent_seconds'] = df['event_params__time_spent'] # just renaming for clarity\n",
    "print(\"Date and time cleanup and transformation completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebd52842",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-based features added successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Add Time-Based Features ---\n",
    "df['ts_weekday'] = df['event_datetime'].dt.day_name() # weekday name\n",
    "df['ts_weekday'] = pd.Categorical(df['ts_weekday'], \n",
    "                                  categories=['Monday', 'Tuesday', 'Wednesday', \n",
    "                                              'Thursday', 'Friday', 'Saturday', \n",
    "                                              'Sunday'],\n",
    "                                  ordered=True) # order the weekdays\n",
    "df['ts_local_time'] = df['event_datetime'] + pd.to_timedelta(df['device__time_zone_offset_hours'].fillna(0), unit='h') # local time\n",
    "df['ts_hour'] = df['ts_local_time'].dt.hour # local hour\n",
    "df['ts_daytime_named'] = df['ts_hour'].apply(lambda x: \n",
    "                                             'Gece' if (x < 6 or x > 22) else \n",
    "                                             'Sabah' if x < 11 else \n",
    "                                             'Öğle' if x < 14 else \n",
    "                                             'Öğleden Sonra' if x < 17 else \n",
    "                                             'Akşam') # time group of day\n",
    "df['ts_is_weekend'] = df['ts_weekday'].apply(lambda x: \n",
    "                                             'Hafta Sonu' if x in ['Saturday', 'Sunday'] else\n",
    "                                             'Hafta İçi') \n",
    "df['ts_weekday'] = df['ts_weekday'].astype(str) # convert to string for consistency\n",
    "print(\"Time-based features added successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38eadd3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session IDs assigned and durations calculated for 381 unique sessions.\n"
     ]
    }
   ],
   "source": [
    "# --- Session Definition and Duration Calculation ---\n",
    "''' \n",
    "Create a calculated session times dataframe from the events dataframe.\n",
    "This will infer session times based on the time gaps between events for each user.\n",
    "\n",
    "This is done by:\n",
    "1. Sorting events by user and timestamp.\n",
    "2. Calculating the time difference between consecutive events for each user.\n",
    "3. Defining a session timeout (6 minutes).\n",
    "4. Assigning session IDs based on the time gaps.\n",
    "'''\n",
    "# Ensure events are sorted per user\n",
    "df_sorted = df.sort_values(by=['user_pseudo_id', 'event_datetime'])\n",
    "# Compute time gap between events per user\n",
    "df_sorted['time_diff'] = df_sorted.groupby('user_pseudo_id')['event_datetime'].diff()\n",
    "# Use 6-minute timeout\n",
    "SESSION_TIMEOUT = pd.Timedelta(minutes=6)\n",
    "# Define inferred session ID using 6-minute gaps\n",
    "df_sorted['inferred_session_id'] = (\n",
    "    (df_sorted['time_diff'] > SESSION_TIMEOUT) | df_sorted['time_diff'].isna()\n",
    ").cumsum()\n",
    "# Assign session IDs to the original DataFrame\n",
    "df['inferred_session_id'] = df_sorted['inferred_session_id'].loc[df.index]\n",
    "# Calculate session duration\n",
    "df['session_duration_seconds'] = df.groupby(['user_pseudo_id', 'inferred_session_id'])['event_datetime'].transform(\n",
    "    lambda x: (x.max() - x.min()).total_seconds()\n",
    ").round(3)\n",
    "df['session_duration_minutes'] = (df['session_duration_seconds'] / 60).round(2)\n",
    "df['session_duration_hours'] = (df['session_duration_seconds'] / 3600).round(3)\n",
    "# Session start and end times\n",
    "df['session_start_time'] = df.groupby(['user_pseudo_id', 'inferred_session_id'])['event_datetime'].transform('min')\n",
    "df['session_end_time'] = df.groupby(['user_pseudo_id', 'inferred_session_id'])['event_datetime'].transform('max')\n",
    "print(f\"Session IDs assigned and durations calculated for {df['inferred_session_id'].nunique()} unique sessions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98feaf40",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character names, tiers, and question indices forward-filled for 381 unique sessions.\n"
     ]
    }
   ],
   "source": [
    "# Infer and forward-fill the character name, current tier, and current question index within each session\n",
    "# Step 1: Sort chronologically within sessions\n",
    "df_sorted = df.sort_values(by=['user_pseudo_id', 'inferred_session_id', 'event_datetime'])\n",
    "# Step 2: Forward-fill the relevant columns per user-session group\n",
    "cols_to_fill = [\n",
    "    'event_params__character_name',\n",
    "    'event_params__current_tier',\n",
    "    'event_params__current_qi',\n",
    "]\n",
    "df_sorted[cols_to_fill] = (\n",
    "    df_sorted\n",
    "    .groupby(['user_pseudo_id', 'inferred_session_id'])[cols_to_fill]\n",
    "    .ffill()\n",
    ")\n",
    "df.loc[df_sorted.index, cols_to_fill] = df_sorted[cols_to_fill]\n",
    "print(f\"Character names, tiers, and question indices forward-filled for {df['inferred_session_id'].nunique()} unique sessions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41906350",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question index cleaned up for 30052 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Question Index Clean-up ---\n",
    "\"\"\"\n",
    "Tier 1: 16 Questions, Except t: 12\n",
    "Tier 2: 12 Questions\n",
    "Tier 3: 12 Questions\n",
    "Tier 4: 10 Questions\n",
    "\"\"\"\n",
    "df['event_params__current_question_index'] = pd.NA\n",
    "df['event_params__current_tier'] = pd.to_numeric(df['event_params__current_tier'], errors='coerce').astype(\"Int64\")\n",
    "df['event_params__current_qi'] = pd.to_numeric(df['event_params__current_qi'], errors='coerce').astype(\"Int64\")\n",
    "notna_mask = df['event_params__character_name'].notna() & df['event_params__current_tier'].notna() & df['event_params__current_qi'].notna()\n",
    "# Tier 1\n",
    "tier_1_mask = notna_mask & (df['event_params__current_tier'] == 1)\n",
    "t_char_mask = tier_1_mask & (df['event_params__character_name'] == 't')\n",
    "df.loc[t_char_mask, 'event_params__current_question_index'] = 13 - df.loc[t_char_mask, 'event_params__current_qi']\n",
    "df.loc[~t_char_mask & tier_1_mask, 'event_params__current_question_index'] = 17 - df.loc[(~t_char_mask) & tier_1_mask, 'event_params__current_qi']\n",
    "# Tier 2 & 3\n",
    "tier_2_3_mask = notna_mask & df['event_params__current_tier'].isin([2, 3])\n",
    "df.loc[tier_2_3_mask, 'event_params__current_question_index'] = 13 - df.loc[tier_2_3_mask, 'event_params__current_qi']\n",
    "# Tier 4\n",
    "tier_4_mask = notna_mask & (df['event_params__current_tier'] == 4)\n",
    "df.loc[tier_4_mask, 'event_params__current_question_index'] = 11 - df.loc[tier_4_mask, 'event_params__current_qi']\n",
    "# Hiccups\n",
    "problems_mask = notna_mask & ~df['event_params__current_tier'].isin([1, 2, 3, 4])\n",
    "if df[problems_mask].shape[0] > 0:\n",
    "    print(\"Something wrong in:\")\n",
    "    print(df.loc[problems_mask, ['event_params__character_name', 'event_params__current_tier', 'event_params__current_qi']])\n",
    "print(f\"Question index cleaned up for {df['event_params__current_question_index'].notna().sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43980479",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative question index calculated for 30052 rows.\n"
     ]
    }
   ],
   "source": [
    "# Calculate cumulative question index\n",
    "df['cumulative_question_index'] = df['event_params__current_question_index'].copy()\n",
    "df['cumulative_question_index'] = pd.to_numeric(df['cumulative_question_index'], errors='coerce')\n",
    "# Tier 2\n",
    "df.loc[(df['event_params__current_tier'] == 2) & (df['event_params__character_name'] == 't'), 'cumulative_question_index'] += 12\n",
    "df.loc[(df['event_params__current_tier'] == 2) & (df['event_params__character_name'] != 't'), 'cumulative_question_index'] += 16\n",
    "# Tier 3\n",
    "df.loc[(df['event_params__current_tier'] == 3) & (df['event_params__character_name'] == 't'), 'cumulative_question_index'] += 24\n",
    "df.loc[(df['event_params__current_tier'] == 3) & (df['event_params__character_name'] != 't'), 'cumulative_question_index'] += 28\n",
    "# Tier 4\n",
    "df.loc[(df['event_params__current_tier'] == 4) & (df['event_params__character_name'] == 't'), 'cumulative_question_index'] += 36\n",
    "df.loc[(df['event_params__current_tier'] == 4) & (df['event_params__character_name'] != 't'), 'cumulative_question_index'] += 40\n",
    "# NaNs\n",
    "df.loc[df['event_params__current_tier'].isna(), 'cumulative_question_index'] = pd.NA\n",
    "print(f\"Cumulative question index calculated for {df['cumulative_question_index'].notna().sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17b7c938",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted maze hand data for 112 rows.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO mini_game_ri\n",
    "\"\"\"\n",
    "# Split 'event_params_mini_game_ri' maze_hand_* into columns\n",
    "# e.g 'maze_hand_WomanHandTwo_maze_level_3'\n",
    "# Column to process\n",
    "col = 'event_params__mini_game_ri'\n",
    "# Filter rows starting with 'maze_hand'\n",
    "mask = df[col].str.startswith('maze_hand', na=False)\n",
    "# Split the matching rows by underscore\n",
    "parts = df.loc[mask, col].str.split('_', expand=True)\n",
    "# Extract Gender and Hand using the updated regex\n",
    "gender_hand = parts[2].str.extract(r'(?P<Gender>Woman|Man)Hand(?P<Hand>\\w+)')\n",
    "# Extract Level (assumed to be in the last part)\n",
    "levels = parts[5]\n",
    "# Create new columns with extracted data\n",
    "df.loc[mask, 'maze_gender'] = gender_hand['Gender']\n",
    "df.loc[mask, 'maze_hand'] = gender_hand['Hand']\n",
    "df.loc[mask, 'maze_level'] = levels\n",
    "print(f\"Extracted maze hand data for {mask.sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1f262b4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted buff data for 167 rows.\n"
     ]
    }
   ],
   "source": [
    "# Split event_params_mini_game_ri buff_* into columns\n",
    "# e.g. 'buff_IncreaseXEnergy_gift_True_gold_False'\n",
    "# Column to process\n",
    "col = 'event_params__mini_game_ri'\n",
    "# Filter rows starting with 'buff'\n",
    "mask = df[col].str.startswith('buff', na=False)\n",
    "# Split the matching rows by underscore\n",
    "parts = df.loc[mask, col].str.split('_', expand=True)\n",
    "# Extract Buff Type and Level\n",
    "buff_type = parts[2].str.extract(r'(?P<BuffType>\\w+)')\n",
    "# Extract Buff Gift and Gold status\n",
    "buff_gift = parts[3].str.extract(r'(?P<BuffGift>\\w+)')\n",
    "buff_gold = parts[5].str.extract(r'(?P<BuffGold>\\w+)')\n",
    "# Create new columns with extracted data\n",
    "df.loc[mask, 'buff_type'] = buff_type['BuffType']\n",
    "df.loc[mask, 'buff_gift'] = buff_gift['BuffGift'].str.lower() == 'true'\n",
    "df.loc[mask, 'buff_gold'] = buff_gold['BuffGold'].str.lower() == 'true'\n",
    "print(f\"Extracted buff data for {mask.sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71da9a5e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted earned buff data for 30 rows.\n"
     ]
    }
   ],
   "source": [
    "# Split event_params_mini_game_ri earned_buff_* into columns\n",
    "# e.g. 'earned_buff_GiveXCharacter'\n",
    "# Column to process\n",
    "col = 'event_params__mini_game_ri'\n",
    "# Filter rows starting with 'earned_buff'\n",
    "mask = df[col].str.startswith('earned_buff', na=False)\n",
    "# Split the matching rows by underscore\n",
    "parts = df.loc[mask, col].str.split('_', expand=True)\n",
    "# Extract Buff Type\n",
    "buff_type = parts[2].str.extract(r'(?P<BuffType>\\w+)')\n",
    "# Create new columns with extracted data\n",
    "df.loc[mask, 'earned_buff_type'] = buff_type['BuffType']\n",
    "print(f\"Extracted earned buff data for {mask.sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38bb9536",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted doll data for 44 rows.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "END mini_game_ri\n",
    "\"\"\"\n",
    "# Split event_params__spent_to doll values into columns\n",
    "# e.g. 'erjohndoll'\n",
    "# Column to process\n",
    "col = 'event_params__spent_to'\n",
    "# Filter rows including string 'doll'\n",
    "mask = df[col].str.contains('doll', na=False)\n",
    "# Split the string by name and doll\n",
    "parts = df.loc[mask, col].str.split('doll', expand=True)\n",
    "# Extract the doll name\n",
    "df.loc[mask, 'doll_name'] = parts[0].str.strip()  # Get the name before 'doll'\n",
    "# Rewrite the 'event_params__spent_to' column to just the doll name\n",
    "df.loc[mask, col] = 'Doll'\n",
    "print(f\"Extracted doll data for {mask.sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0caf81a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted crystal ball data for 228 rows.\n"
     ]
    }
   ],
   "source": [
    "# Split event_params__spent_to crystal values into columns\n",
    "# list of possible values: cauldron_item, aliginn_item, coffee_item\n",
    "# Column to process\n",
    "col = 'event_params__spent_to'\n",
    "# Filter rows including values from the list\n",
    "mask = df[col].str.contains('cauldron_item|aliginn_item|coffee_item', na=False)\n",
    "# Split the string by name and item\n",
    "parts = df.loc[mask, col].str.split('_', expand=True)\n",
    "# Extract the item name\n",
    "df.loc[mask, 'spent_in_crystal'] = parts[0].str.strip()  # Get the name before '_item'\n",
    "# Rewrite the 'event_params__spent_to' column to just the item name\n",
    "df.loc[mask, col] = 'Crystal Ball'\n",
    "print(f\"Extracted crystal ball data for {mask.sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48ee5045",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted permanent shop item data for 92 rows.\n"
     ]
    }
   ],
   "source": [
    "# Write event_params_spent_to permanent shop item values into shop_permanent_item\n",
    "# list of possible values: dreamcatcher, catcollar, library1, library2, bugspray, schedule\n",
    "# Column to process\n",
    "col = 'event_params__spent_to'\n",
    "# Filter rows including values from the list\n",
    "mask = df[col].str.contains('dreamcatcher|catcollar|library1|library2|bugspray|schedule|crystal|horseshoe', na=False)\n",
    "# Create a new column for the shop permanent item\n",
    "df.loc[mask, 'shop_permanent_item'] = df.loc[mask, col].str.extract(r'(dreamcatcher|catcollar|library1|library2|bugspray|schedule|crystal|horseshoe)')[0]\n",
    "# Rewrite the 'event_params__spent_to' column to just the item name\n",
    "df.loc[mask, col] = 'Permanent Item'\n",
    "print(f\"Extracted permanent shop item data for {mask.sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02d6174e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted consumable shop item data for 166 rows.\n"
     ]
    }
   ],
   "source": [
    "# Write event_params_spent_to consumable shop item values into shop_consumable_item\n",
    "# list of possible values: potion, ıncense, amulet, incense\n",
    "# Column to process\n",
    "col = 'event_params__spent_to'\n",
    "# Filter rows including values from the list\n",
    "mask = df[col].str.contains('potion|ıncense|amulet|incense', na=False)\n",
    "# Create a new column for the shop consumable item\n",
    "df.loc[mask, 'shop_consumable_item'] = df.loc[mask, col].str.extract(r'(potion|ıncense|amulet|incense)')[0]\n",
    "# Rewrite the 'event_params__spent_to' column to just the item name\n",
    "df.loc[mask, col] = 'Consumable Item'\n",
    "print(f\"Extracted consumable shop item data for {mask.sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81da6b90",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted board item data for 376 rows.\n"
     ]
    }
   ],
   "source": [
    "# Write event_params_spent_to mini_game remainin item values into board_item\n",
    "# everything except: ['Doll', 'Crystal Ball', 'Permanent Item', 'Consumable Item']\n",
    "# Column to process\n",
    "col = 'event_params__spent_to'\n",
    "# Filter rows that are not in the known categories\n",
    "mask = (~df[col].isin(['Doll', 'Crystal Ball', 'Permanent Item', 'Consumable Item'])) & \\\n",
    "    (df['event_params__where_its_spent'].isin(['board', 'board_item']))\n",
    "# Create a new column for the board item\n",
    "df.loc[mask, 'board_item'] = df.loc[mask, col]\n",
    "# Rewrite the 'event_params__spent_to' column to just the item name\n",
    "df.loc[mask, col] = 'Board Item'\n",
    "print(f\"Extracted board item data for {mask.sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40c9c429",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 30 columns: ['event_timestamp', 'event_previous_timestamp', 'user_first_touch_timestamp', 'device__time_zone_offset_seconds', 'event_params__engagement_time_msec', 'event_previous_datetime', 'event_params__time_spent', 'event_first_touch_datetime', 'user__first_open_datetime', 'event_value_in_usd', 'user_id', 'batch_page_id', 'batch_ordering_id', 'privacy_info__uses_transient_token', 'user_ltv', 'device__mobile_marketing_name', 'device__vendor_id', 'device__browser', 'device__browser_version', 'device__web_info', 'event_dimensions', 'traffic_source__name', 'traffic_source__medium', 'traffic_source__source', 'ecommerce', 'event_server_timestamp_offset', 'event_params__update_with_analytics', 'event_params__system_app_update', 'collected_traffic_source', 'event_params__system_app'].\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=columns_to_drop)\n",
    "print(f\"Dropped {len(columns_to_drop)} columns: {columns_to_drop}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91fc4194",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying value maps to the DataFrame...\n",
      "Value maps applied. DataFrame now has 102 columns.\n"
     ]
    }
   ],
   "source": [
    "# Rewrite the 'key' value in 'event_params__spent_to' as 'Key'\n",
    "df.loc[df['event_params__spent_to'] == 'key', 'event_params__spent_to'] = 'Key'\n",
    "# Apply value maps to the DataFrame\n",
    "print(\"Applying value maps to the DataFrame...\")\n",
    "df = apply_value_maps(df, map_of_maps, keep_unmapped=True)\n",
    "print(f\"Value maps applied. DataFrame now has {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "661c9bcc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question address created for 30052 rows.\n"
     ]
    }
   ],
   "source": [
    "# Create adressable question index\n",
    "df['question_address'] = df['event_params__character_name'] + ' - T: ' + df['event_params__current_tier'].astype(str) + ' - Q: ' + df['event_params__current_question_index'].astype(str)\n",
    "print(f\"Question address created for {df['question_address'].notna().sum()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beb95b2a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User metrics calculated for 41 users.\n"
     ]
    }
   ],
   "source": [
    "# Create user_metrics\n",
    "df['event_datetime'] = pd.to_datetime(df['event_datetime'], errors='coerce')\n",
    "# Group by user and calculate user-level metrics\n",
    "user_metrics = df.groupby('user_pseudo_id').agg(\n",
    "    first_seen=('event_datetime', 'min'),\n",
    "    last_seen=('event_datetime', 'max'),\n",
    "    total_sessions=('inferred_session_id', pd.Series.nunique),\n",
    "    total_events=('event_name', 'count')\n",
    ").reset_index()\n",
    "# Reference date: typically the latest timestamp in your data\n",
    "reference_date = df['event_datetime'].max()\n",
    "# Lifetime: just for info\n",
    "user_metrics['lifetime_days'] = (user_metrics['last_seen'] - user_metrics['first_seen']).dt.days\n",
    "# Days since last activity\n",
    "user_metrics['days_since_last_seen'] = (reference_date - user_metrics['last_seen']).dt.days\n",
    "# Churn: hasn't been seen for 14+ days\n",
    "user_metrics['is_churned'] = user_metrics['days_since_last_seen'] > 14\n",
    "# Active/returning user flags\n",
    "user_metrics['is_retained_1d'] = user_metrics['lifetime_days'] >= 0\n",
    "user_metrics['is_retained_7d'] = user_metrics['lifetime_days'] >= 7\n",
    "user_metrics['is_retained_30d'] = user_metrics['lifetime_days'] >= 30\n",
    "# Calculate active days: number of unique days the user has been active\n",
    "days_active = df.groupby('user_pseudo_id')['event_datetime'].apply(\n",
    "    lambda x: x.dt.normalize().nunique()\n",
    ").reset_index(name='active_days')\n",
    "user_metrics = user_metrics.merge(days_active, on='user_pseudo_id', how='left')\n",
    "# Active user flags based on active days\n",
    "user_metrics['is_active_1d'] = user_metrics['active_days'] >= 2\n",
    "user_metrics['is_active_7d'] = user_metrics['active_days'] >= 7\n",
    "user_metrics['is_active_30d'] = user_metrics['active_days'] >= 30\n",
    "# Is active yesterday: if the user has been seen in the last 24 hours\n",
    "user_metrics['is_active_yesterday'] = user_metrics['days_since_last_seen'] <= 1\n",
    "# User status based on activity\n",
    "conditions = [\n",
    "    user_metrics['days_since_last_seen'] > 14,         # churned first\n",
    "    user_metrics['days_since_last_seen'] <= 7,         # then active\n",
    "    user_metrics['lifetime_days'] <= 1                 # then new\n",
    "]\n",
    "labels = ['Bırakmış', 'Aktif', 'Yeni']\n",
    "user_metrics['user_status'] = np.select(conditions, labels, default='dormant')\n",
    "df = df.merge(user_metrics, on='user_pseudo_id', how='left')\n",
    "print(f\"User metrics calculated for {user_metrics.shape[0]} users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3332bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate app-version-level KPIs\n",
    "kpis_per_version = []\n",
    "\n",
    "for version, group in df.groupby('app_info__version'):\n",
    "    # Drop duplicates to get one row per user\n",
    "    users = group[['user_pseudo_id', 'first_seen', 'last_seen', 'lifetime_days',\n",
    "                   'days_since_last_seen', 'active_days']].drop_duplicates()\n",
    "\n",
    "    # User-level flags\n",
    "    users['is_churned'] = users['days_since_last_seen'] > 14\n",
    "    users['is_retained_1d'] = users['lifetime_days'] >= 0\n",
    "    users['is_retained_7d'] = users['lifetime_days'] >= 7\n",
    "    users['is_retained_30d'] = users['lifetime_days'] >= 30\n",
    "    users['is_active_1d'] = users['active_days'] >= 2\n",
    "    users['is_active_7d'] = users['active_days'] >= 7\n",
    "    users['is_active_30d'] = users['active_days'] >= 30\n",
    "    users['is_active_yesterday'] = users['days_since_last_seen'] <= 1\n",
    "\n",
    "    # User status\n",
    "    users['user_status'] = np.select(\n",
    "        [\n",
    "            users['days_since_last_seen'] > 14,\n",
    "            users['days_since_last_seen'] <= 7,\n",
    "            users['lifetime_days'] <= 1\n",
    "        ],\n",
    "        ['Bırakmış', 'Aktif', 'Yeni'],\n",
    "        default='dormant'\n",
    "    )\n",
    "\n",
    "    # Aggregate KPIs\n",
    "    kpis_per_version.append({\n",
    "        'app_version': version,\n",
    "        'report_days': (reference_date - group['event_datetime'].min()).days,\n",
    "        'report_date': reference_date.normalize(),\n",
    "        'report_timestamp': reference_date,              # for versioning/debugging/audit\n",
    "        'total_users': users.shape[0],\n",
    "        'total_sessions': group['inferred_session_id'].nunique(),\n",
    "        'sessions_per_player': group['inferred_session_id'].nunique() / users.shape[0],\n",
    "        'total_events': group.shape[0],\n",
    "        'churn_rate': users['is_churned'].mean(),\n",
    "        'retention_1d': users['is_retained_1d'].mean(),\n",
    "        'retention_7d': users['is_retained_7d'].mean(),\n",
    "        'retention_30d': users['is_retained_30d'].mean(),\n",
    "        'active_1d': users['is_active_1d'].mean(),\n",
    "        'active_7d': users['is_active_7d'].mean(),\n",
    "        'active_30d': users['is_active_30d'].mean(),\n",
    "        'active_yesterday': users['is_active_yesterday'].mean(),\n",
    "        'ads_shown': group['event_name'].str.contains('Ad Impression', case=False, na=False).sum(),\n",
    "        'ads_per_session': group['event_name'].str.contains('Ad Impression', case=False, na=False).sum() / group['inferred_session_id'].nunique(),\n",
    "        'ads_per_player': group['event_name'].str.contains('Ad Impression', case=False, na=False).sum() / users.shape[0],\n",
    "        'ads_per_active_player': group['event_name'].str.contains('Ad Impression', case=False, na=False).sum() / users[users['is_active_yesterday']].shape[0],\n",
    "        'new_users': (users['lifetime_days'] <= 1).sum(),\n",
    "        'returning_players': ((users['lifetime_days'] > 1) & (users['days_since_last_seen'] <= 7)).sum(),\n",
    "        'churned_players': (users['is_churned']).sum(),\n",
    "    })\n",
    "\n",
    "# Combine all KPIs into a DataFrame\n",
    "kpis_df = pd.DataFrame(kpis_per_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8599cc8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting boolean columns to integers...\n",
      "Boolean columns converted. DataFrame now has 119 columns and user_metrics has 17 columns.\n"
     ]
    }
   ],
   "source": [
    "# Convert boolean columns to integers (0/1)\n",
    "print(\"Converting boolean columns to integers...\")\n",
    "df = convert_bool_to_int(df)\n",
    "user_metrics = convert_bool_to_int(user_metrics)\n",
    "print(f\"Boolean columns converted. DataFrame now has {df.shape[1]} columns and user_metrics has {user_metrics.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1b78dc3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming and selecting columns according to df_column_names_map...\n",
      "DataFrame columns renamed and selected according to the map. Now has 103 columns.\n"
     ]
    }
   ],
   "source": [
    "# Rename and select columns according to df_column_names_map\n",
    "print(\"Renaming and selecting columns according to df_column_names_map...\")\n",
    "df = safe_select_and_rename(df, df_column_names_map)\n",
    "print(f\"DataFrame columns renamed and selected according to the map. Now has {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20d40a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to CSV files based on the splits defined in df_splits\n",
    "print(\"Saving DataFrame splits to CSV files...\")\n",
    "for name, cols in df_splits.items():\n",
    "    df_subset = df[cols].copy()\n",
    "    if name in df_filters:\n",
    "        df_subset = df_subset[df_filters[name](df_subset)]\n",
    "    df_subset.to_csv(f'./data/{name}.csv', index=False)\n",
    "print(\"Data cleaning and transformation completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68876270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving aggregated dataframes to CSV files...\")\n",
    "KPI_PATH = './data/kpis_df.csv'\n",
    "# Load existing KPI data if exists\n",
    "if os.path.exists(KPI_PATH):\n",
    "    existing_kpis = pd.read_csv(KPI_PATH)\n",
    "    existing_kpis['report_date'] = pd.to_datetime(existing_kpis['report_date'])\n",
    "    file_exists = True\n",
    "else:\n",
    "    existing_kpis = pd.DataFrame()\n",
    "    file_exists = False\n",
    "# Filter out duplicates\n",
    "kpis_df = pd.DataFrame(kpis_per_version)\n",
    "if not existing_kpis.empty:\n",
    "    kpis_df = kpis_df.merge(\n",
    "        existing_kpis[['app_version', 'report_date']],\n",
    "        on=['app_version', 'report_date'],\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    ).query('_merge == \"left_only\"').drop(columns=['_merge'])\n",
    "# Append only new rows\n",
    "if not kpis_df.empty:\n",
    "    kpis_df.to_csv(\n",
    "        KPI_PATH,\n",
    "        mode='a' if file_exists else 'w',\n",
    "        header=not file_exists,\n",
    "        index=False\n",
    "    )\n",
    "print(\"Cleaned data saved to CSV files successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fcc278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "print(\"Saving seperate dataframes to CSV files...\")\n",
    "df.to_csv('./data/cleaned_data.csv', index=False, chunksize=100000)\n",
    "kpis_df.to_csv('./data/kpis_df.csv', index=False)\n",
    "print(\"Cleaned data saved to CSV files successfully.\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a492885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "jupytext_format_version"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
